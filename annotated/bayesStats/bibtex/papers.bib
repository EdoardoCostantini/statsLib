%%% Title:    Mathematical Foundation Books
%%% Author:   Edoardo Costantini
%%% Created:  2019-12-10
%%% Modified: 2020-04-02

@article{chipman:1998,
 annote=
 	{
 	The authors propose a Bayesian approach for finding of classification and regression trees that outperforms, mainly 
	in terms of predictive performance, trees grown according to more traditional routines.
		This approach has two components:
		\begin{itemize}
		\item a prior specification for the set of possible CART models, which entails the specification
			of a prior for the tree space and one for the terminal nodes of each tree.
			\begin{itemize}
		\item the specification of a prior probability that terminal node $\eta$ is split ($p_{SPLIT}$), and the probability of
			assigning splitting rule $\rho$ to $\eta$ if it is split ($p_{RULE}$). The prior assigned to $p_{SPLIT}$, depends on
			the number of splits above $\eta$ in a way that favours trees that have terminal nodes that do not vary much in
			depth (i.e. number of splits above). The prior for $p_{RULE}$ is a uniform specification: all the variables have 
			uniform probability of being chosen as splitting variables, and all the observed values on these variables have the
			 same probability of being chosen as splitting values.
		\item priors for terminal nodes parameters (e.g. vector of terminal node means) are usually specified assuming 
			independence of terminal node parameters across terminal nodes. For regression trees, one could use 
			normal priors centred around some shared mean value with a shared (or not) error variance; for classification
			trees Dirichlet distributions (multivariate generalisation of a beta prior) are recommended.
			\end{itemize}
		\item a stochastic search of a promising CART (guided by the prior) through a Metropolis-Hastings
			search algorithm. After deciding on an initial tree, the proposal tree is obtained through one of four strategies
			(randomly selected): grow, prune, change, swap (see paper for details) and an acceptance ratio is computed
			based on the current and new tree draw. Such procedure simulates a Markov Chain sequence of trees that
			converges to the tree posterior distribution of interest $P(T|Y,X)$.
		\end{itemize}	
	}
 }

@article{chipman:2010,
 annote=
 	{
 	The authors propose and ensemble method for approximating the expected value of some continuous and dichotomous Y, conditioned on
		a p-dimensional X matrix of predictors, by a sum of $m$ regression trees. This Bayesian Additive Regression Tree
		method uses a sum-of-trees to approximate $f(x)=E(Y|X)$ and it does so by imposing a regularising prior that weakens
		 each of the $m$ trees making up the sum-of-trees. By doing so, each tree part of the sum is explaining a small but 
		 unique portion of $f$, and in this sense BART is not the same as averaging single trees fitted to approximate
		 $f$, as boosting, bagging and random forests do.
	A BART model is made up of two fundamental parts:
	\begin{itemize}
		\item the sum-of-trees model
		$ Y = \Sigma_{j=1}^{m} g(x; T_j, M_j)+\epsilon $ with $\epsilon \sim N(0, \sigma^2)$
		where $g(...)$ is the function that assigns one of the terminal node parameters in $M_j$ of tree $T_j$ to all observation in
		$x$
		\item a (set of) regularising prior(s) for all of the parameters in the sum-of-trees model: all of the $T_j$ and $M_j$, and
		 $\sigma$. These priors keep the effect of each individual tree small. Interestingly: the $T_j$ (independent) prior is operationalised
		 exactly as for for the Bayesian CART in Chipman 1998 (see equation 7 of both papers) in terms of a the probability that a split
		 is non terminal ($p_{SPLIT}=\alpha(1+d)^{\beta}$) and uniform probabilites of selecting a variable as a splitting one and an
		 observed value as a splitting value; the prior for the components of $M_j$, $\mu_{ij}$ are normal priors with shared values for
		 the mean and variance hyperparameters, which ends up being a specific version of the standard conjugate normal prior specified
		 in equation 9 of Chipman 1998: finally the prior for $\sigma**2$ is also the same the one specified for the Bayesian CART, just 
		 expressed with the alternative inverse-chi-square parametrisation.
		 \item choice of $m$ - BART needs to a fixed number of $m$ trees to be defined. This number is usually required to be high for 
		 prediction tasks, but when small (say 5 to 10) makes BART an interesting variable selection routine.
	\end{itemize}
	Compared to Chipman 1998, BART produces a posterior probability of a collection of $m$ different trees conditioned on a dataset (Y,X) 
		$p((T_1,M_1), ...,  T_j,M_j), ..., T_m,M_m), \sigma^2|Y,X)$, instead of the posterior probability of one single tree $P(T|Y,X)$. 
		However, the draws of each $(T_j, M_j)$ can be done by drawing $T_j$ with the Metropolisâ€“Hastings algorithm defined in Chipman
		1998 using a modified version of the data (y minus the fit from the sum of trees excluding the current tree), and the draws of $M_j$ 
		can be done from normal distributions.
	By iterating K times, meaning repeating the draw of all of the $m$ trees and $\sigma^2$ K times, we obtain a sequnce of $f^{*}$ draws that
		approximates the posterior disitrbution of $p(f|y)$, whatever $f(.)$ is.
	}
}

@inbook{dagpunar:2007,
  annote = 
  	{
	It provides a very clear description of rejection sampling with clear (and reproducible) 
	examples. It is particularly helpful to understand the interplay of the envelope and squeezing
	functions. The section dedicated to rejection sampling is also insightful.
  	}
}

@article{gelman:1993,
  annote=
  	{
	A clarification on the conditions under which a set of conditional (and marginal) distributions 
  		uniquely specifies a joint distribution.
	}
}

@article{gelmanRaghunthan:2001,
 annote = 
	{
	The authors discuss the use of conditional distributions not to approximate joint models
		but for the purpose of multiple imputation. This is simply a short compendium 
		that facilitate the understanding of SRMI/FCS Multiple Imputation for someone 
		coming from a more traditional Bayesian background.
	}
}

@article{gilksWild:1992,
 annote = 
	{
	Main reference describing the use of adaptive rejection sampling to sample from
	log-concave full conditional distributions.
	For a good description of ARS, look at books by Dagpunar (2007) and Giudici et al
	(2007), also described in this archive.
	}
}

@article{gilksEtAl:1995,
 annote = 
	{
	Main reference describing the use of adaptive rejection sampling to sample from 
	non-log-concave full conditional distributions (generalisation of Gilks and Wilds (1992)).
	}
}

@article{griffinBrown:2005,
 annote = 
	{
	The authors present scale mixture of normal prior distributions that can be used to
		perform variable selection in Bayesian regression analysis, laying the groundwork
		for the development of actual bayesian counterparts to the frequentist Lasso
		penalty for regression coefficient estimates. \\
		
	This paper describes clearly how the mixture of normal distribution priors proposed by 
		Griffin and Brown (2011), to achieve some form of lasso regularisation in a bayesian 
		framework, can be considered as generalisation of the one proposed by Park and 
		Casella (2008).
	}
}

@article{griffinBrown:2011,
 annote = 
	{
	With this study, the authors propose priors for Bayesian Lasso that are generalisations
		of Park and Casella's (2008) one.
	}
}

@inbook{giudiciEtAl:2013,
 	{
	Contains detailed description of two important sampling methods for random variates:
	(1) the inverse cumulative distribution method, and (2) (adaptive) rejection sampling.
	It is particularly helpful to see how to derive a CDF from a PDF and use it to sample for sampling,
	and how to generate samples from a piecewise linear (or exponential) envelope.
	}  
}

@article{hans:2009,
  annote = 
	{
	A new Gibbs sampler is proposed for Bayesian Lasso regression.
	}
}

@article{hans:2010,
 annote = 
	{
	Extends the Bayesian lasso Gibbs sampler proposed in Hans (2009) to account for 
	large numbers of predictors	
	}
}


@article{kyung:2010,
 annote=
 	{
	A comprehensive description of the Bayesian lasso and its frequentists counterparts.
	}
}

@article{liYao:2018,
 annote = 
	{
	The author implements a multinomial bayesian regression with lasso priors by proposing
		a new t-prior and adapting the priors proposed by Griffin and Brown (2011) for variable
		selection. The paper was developed along with an R-package (HTLR) that can be used to
		perform analyses and predictions.
	}
}

@article{parkCasella:2008,
 annote=
 	{
	The authors propose a conditional Laplace prior for the regression coefficients of a bayesian regression
		model that produce posterior mode estimates that have the same interpretation as Lasso
		estimates.
	}
}