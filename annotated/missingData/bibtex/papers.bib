%%% Title:    Missing Data Journal Articles: Annotations
%%% Author:   Edoardo Costantini, Kyle M. Lang, & Stats-Refs Contributors
%%% Created:  2019-09-13
%%% Modified: 2019-12-04

@article{anderson:1957,
annote = 
	{
	This article provides a concise mathematical workout of Maximum Likelihood estimation of multivariate
		normal models with a monotone missing data pattern (from bivariate to any-variate generalisation).
	}
}

@article{andridgeLittle:2010,
annote=
	{
	Andrige and Little point out two main advantages of hot deck imputation methods: it results in a 
		rectangular data and it does not rely on any model specification. Different measures and methods
		for creating the donor pool are reviewed along with how to account for missing data patterns 
		(monotone or \textit{Swiss cheese}) and how to incorporate sampling weights.
	}
}

@article{borgoniBerrington:2013,
annote = 
	{
	The authors integrate a sequential (one-variable-at-the-time + data augmentation) tree-based imputation
		algorithm and non-parametric bootstrap. Specifically, given a set of fully observed inputs and 
		a set of target variables with missing values, their approach grows a tree on a complete subset 
		of the instances for each target variable sequentially (starting with the target variable with 
		fewest missing cases, uses the imputed values to augment the original data, and improve imputation 
		of variables with more missing values), and iterates this procedure until some criteria is reached. 
		To account for the added uncertainty due to imputation, the authors apply this method to a large
		number of bootstrap samples from the original dataset. \\

	Considering only categorical variables, the authors compare mode, sequential regressions, a non-iterative
		tree-based imputations to their bootstrap tree-based imputation algorithm. Most notably, the results 
		of the simulation study point out that non-iterative regression trees are worst than simple
		mode imputation in terms of estimation accuracy (bias and efficiency of a true model parameters
		estimates), and that their bootstrap version of an imputing decision tree compensates well for the
		extra variability added by the imputation process.
	}
}

@article{browningBrowning:2009,
annote = { The authors aim to develop methods for genotype imputation and
                  inference for large data sets of unrelated individuals and
                  parent-offspring trios. A general class of hidden Markov
                  models (HMMs) is proposed to analyse missing data. The authors
                  first present methods for building the haplotype HMM. The
                  iterations of the model-building and samplings step are used
                  to achieve an accurate haplotype HMM. The authors indicate
                  that if the haplotype is weighted in the iterativealgorithm,
                  the model will be more accurate with a smaller amount of
                  iterations. There’s another way of improving the accuracy of
                  the model, which is averaging the posterior genotype
                  probabilities over multiple iterations. The methods are
                  implemented in BEAGLE 3.0 package and BEAGLE is compared with
                  IMPUTE. BEAGLE 3.0 can work with either phased or unphased
                  reference panel while IMPUTE requires a phased reference
                  panel. The authors evaluate the imputation accuracy and
                  computational efficiency through real data sets and simulation
                  studies. The results show that posterior probabilities are
                  well evaluated. For estimating the accuracy of the imputation,
                  the authors also propose a useful method for measuring allelic
                  R 2 .  Finally, the authors conclude that the size of the
                  reference panels is also essential for the imputation
                  accuracy. }
}

@article{burgetteReiter:2010,
annote = 
	{
	The authors propose a multiple imputation via sequential decision trees to deal with large datasets with
		missing data on many predictors. The approach is rooted in the MICE algorithm. The main point
		of departure is the way predictive distribution draws are done. For each target variable with missing values
		a tree is grown on the remaining complete (augmented) dataset. The imputations are done by locating
		each case with a missing value on the current target variable in a leaf of such a tree and attributing 
		a randomly sampled target variable value among the observed ones in that leaf. \\

	In the simulation setup, the approach proposed works fine in terms of providing low bias of the parameter estimates
		of the data generating model fitted to the imputed data. However, the coverage rates are alarmingly low.
	}
}

@article{collinsEtAl:2001,
annote = 
	{
	Collins \textit{et al} investigate the effects of an inclusive \textit{vs} a restrictive strategy to the use of
		auxiliary variables in the missing data handling model of either a ML (maximum likelihood) 
		or MI (Multiple Imputation) approach. \\

	The authors distinguish between three categories of auxiliary variables based on their 
		correlation with the variable(s) of interest that presents missing values, and the missingness 
		mechanism). Through 4 different simulation setups, the authors study the
		consequences of including or excluding auxiliary variables, from each of these categories, 
		and ultimately provide compelling evidence to prefer an inclusive strategy. \\

	An interesting point is raised regarding the ease of implementation of the inclusive strategy. In particular, 
		the authors point out that, while the inclusion of auxiliary variables is straightforward in the MI 
		framework, current (up to 2001) software implementations of ML methods do no facilitate it. This topic 
		is thoroughly explored by Graham (2003). \\

	A final point stressed by the article is that the consequences of the inclusive or restrictive strategies 
		heavily depend on the type of MAR. For example, in their simulations, Collins \textit{et al} found 
		that the estimated mean of a variable Y of interest was biased when the auxiliary variable Z was 
		excluded from the data handling model, if the probability of missingess was linearly related to Z 
		(MAR-linear); however, that was not the case when the probability of missigness was larger for 
		extreme values of Z or a function of the correlation between Z and Y.
	}
}

@article{conversanoSiciliano:2009,
annote = 
	{
	The authors introduce the Incremental NonParametric Imputation (INPI) algorithm for imputing large datasets 
		with missing values on multiple categorical and/or continuous variables. This approach reorganises the 
		data according to a lexicographic order (arranging the columns and rows of a data matrix so as to concentrate
		the missing values in a corner), grows a decision tree with a FAST algorithm (selecting first the best
		splitting predictor and then the best splitting cut off value), imputes a missing value that minimises
		a given error/decision rule, and augments the complete data before repeating the process for all the 
		other missing values. \\

	The INPI algorithm exploits all of the advantages of decision tree methods (i.e. nonparametric nature, flexibility
		to variable types) and grants an effective imputation method for large datasets. Another advantage of the 
		is that the algorithm can be easily extended to include imputation model uncertainty (Multiple 
		Imputation). However, it must be noticed that this algorithm assumes a MAR mechanisms and that there 
		is at least one completely observed variable in the original dataset.
	}
}

@article{d'ambrosioEtAl:2012,
annote = 
	{
	The imputation algorithm proposed in this article improves on the INPI algorithm proposed by Conversano
		and Siciliano (2009) in two ways: by incrementally imputing one variable at the time, it reduces 
		the computational effort compared to the imputation of single missing data points one at the time;
		by using the ensemble learning classifier AdaBoost, instead of single decision tree, the new 
		algorithm is more accurate, in terms of prediction error. \\

	This article also integrates this imputation approach in a ``data fusion'' problem. This could be 
		interesting for imputation problems with datasets gathered according to planned missing data 
		designs. \\

	The article interestingly includes the MICE approach as one of the imputation methods compared. However, 
		it does not provide a valid measure of comparison between MICE and BINPI. The authors stress how
		the performance of a BINPI algorithm should be evaluated based on its ability to recover the 
		missing values (according to the Statistical Learning Theory framework). However, MICE is an imputation
		strategy grounded in the stochastic framework proposed by Rubin. Hence, MICE should be 
		evaluated not in terms of the ability to recover missing values (prediction error), but in terms of
		how statistically valid the analysis that it allows to perform are.
	}
}

@article{deAndradeSilvaHruschka:2013,
annote = 
	{
	The article compares 5 popular Nearest-Neighbour algorithms based on both their prediction accuracy and 
		the classification bias. The results show that in a MCAR context, the Iterative KNNImpute algorithm proposed
		by Bràs and Menezes (2007) outperforms all other methods, according to both the Normalised Root Mean Squared 
		Error (NRMSE) and the Average Correct Classification Rate (ACCR) criterion. However, in a MAR scenario,
		the KNNI proposed by Troyanskaya et al 2001, the Sequential KNNI proposed by Kim et al (2004), and the 
		EACI proposed by de Andrade Silva and Hruschka (2009) perform best. \\

	One of the clearest contribution of the article is showing that
		better prediction accuracy does not necessarily imply a better modelling performance (i.e. an algorithm
		might ``recover'' the missing values better, but another algorithm might provide a better estimation
		of the relationship between attributes). This result motivates a departure from the exclusive use of
		the Normalised Root Mean Squared Error measure of prediction accuracy as the criteria to evaluate an imputation
		algorithm. It must be noted that this article is concerned exclusively with a classification task.
	}
}

@article{dempsterEtAl:1977,
annote = 
	{
	The EM algorithm is introduced in this paper as an approach to iteratively 
		compute maximum likelihood estimates when data is incomplete. \\
		
	The algorithm is introduced first through a numerical example involving discrete variables,
		then, formally, for the case in which the complete data distribution belongs to the 
		exponential family. Finally, it is extended for any distributional family.
		
	The article extensively discusses the general properties of the EM algorithm with great
		attention to the technical details. In section 3, the authors provide the mathematical
		justification for the effectiveness of the algorithm in finding the maximum likelihood
		estimate of a vector parameter when the complete-data likelihood is unkwon/intractable.
		
	Finally, the application of the EM algorithm is discursively exemplified in different scenarios
		such as missing data, and mixture modelling.
	}
}

@article{dengEtAl:2016,
annote = 
	{
	The authors carried out a comparison study similar to that of Zhao and Long (2013) in order to assess the efficiency
		of regularised regression for multiple imputation of high-dimensional data. One key contribution is 
		generalising the univariate missing case to multivariate missing patterns. However, the study considers 
		at most three variables afflicted by missing values (at least in the simulation study). \\

	The study shows a clear dominance of MICE-IURR (MICE with indirect use of regularised regression) in terms of bias
		compared to all other viable methods. In terms of coverage rate, there is no clear winner. Notably, the Random
		Forest MICE approach proposed by Shah et al 2014 does not live up to the promising findings of this previous
		study (the unsatisfactory performance of MICE-RF is corroborated by Drechsler and Reiter, 2011). Finally,
		the result shown for MICE-DURR are also in disagreement with what was found by Zhao and Long (2013).
	}
}

@article{dooveEtAl:2014,
annote = 
	{
	The authors present three implementation of recursive partitioning techniques for multiple imputations:
		CART, Random forest on bootstrap samples and Random forest with random feature inclusion. The
		driving principle behind the implementation of these methods in a MICE framework is their ability
		to automatically take into account interaction effects. \\

	Through two simulation studies the authors find that MICE-CART is the one that achieves the lowest bias of
		the interaction effects parameters estimates and is more efficient than both Predictive Mean Matching
		and Random forests. (results presented for both dichotomous and continuous DVs). \\

	This paper does not directly deal with any high-dimensionality issue but clearly presents an implementation
		of MICE CART and Random forests that could be useful in such a context.
	}
}


@article{drechslerReiter:2011,
annote =
	{
	The authors compare four Machine Learning methods to create synthetic datasets: (sequential/repeated) CART, 
		implemented in a similar fashion to Burgette and Reiter (2010); CART applied to bootstrapped subsamples 
		of the original dataset (random forests and bagging); and Support Vector Machines. These prediction 
		methods are expected to generate synthetic datasets that preserve the original complex data structure, 
		and hence grant statistical validity of any secondary analyses carried out on them. \\

	Generating a synthetic dataset can be thought of as imputing a dataset with missing data. The issue of granting 
		statistical validity of the analyses performed on a partially synthetic dataset is the same as that 
		of obtaining statistically valid analyses on an imputed dataset. Therefore, these findings are directly
		applicable to the high-dimensional missing data handling problem. The study shows sequential/MI CART and 
		SVM synthetic dataset generation (imputation) procedures outperform both random forests and bagging 
		approaches.
	}
}

@article{garcia-laencinaEtAl:2009,
annote =
	{
	The authors introduce an improved version of the KNNImpute algorithm proposed by Troyanskaya 
		\textit{et al.} 2001. Their proposed strategy is called MI-KNNImpute, a somewhat confusing
		label: the ``MI'' portion does not refer to ``multiple imputation'' but to the concept of
		Mutual Information (i.e. the reduction of the uncertainty of a variable when another one is
		known). \\
	
	The authors show with different incomplete datasets that the performance of the classification task, performed 
		by a KNN algorithm that uses a distance measure of Euclidian form and one that uses MI, is 
		improved by first imputing the datasets with the MI-KNNImpute algorithm compared to the standard 
		KNNImpute. \\

	The improved performance is achieved by including some information regarding the classification task
		in the imputation phase, through the use of MI as a measure of distance between the target 
		class variable and the other input attributes. The use of MI in the algorithm effectively 
		weights the importance of each attribute for the imputation of the target class variable.
	}
}

@article{graham:2003,
annote =
	{
	The author effectively shows, through multiple simulation studies, that including auxiliary 
		variables, when using a FIML (Full Information Maximum Likelihood) approach to 
		handling and analysing datasets with missing data, can be done relatively easily
		under the structural equation modelling framework. \\

	With this paper, Graham introduces the \textit{Saturated Correlated model} and 
		\textit{the extra dv model} to include auxiliary variables in a SEM model. Ultimately,
		he showed that it is possible to include auxiliary variables in a ML missing data 
		handling procedure, without affecting the substantive model (that is to say obtaining 
		the same parameter estimates, standard errors, and estimates of quality of fit of a 
		substantive model that does not include them).
	}
}

@article{heBelin:2014,
annote = 
	{
	The authors propose a joint modelling imputation approach that accommodates for both continuous and binary variables
		ith missing values.
	}
}

@article{howardEtAl:2015,
annote = 
	{
	The authors propose an ``all-inclusive'' strategy for auxiliary variables that uses PCA to 
		reduce the number of predictors included in the imputation model, while preserving
		as much as possible the variability of all the possible auxiliary variables in a
		dataset. The study tries to establish whether this strategy is as beneficial as a 
		standard inclusive strategy compared to no use of auxiliary variables. \\

	In the authors linear MAR simulation set up, the PCA approach proposed showed no convergence
		failures (i.e. all runs of the algorithm led to stable parameter values and SE, with 
		no impossible values), negligible bias, the same efficiency (measured as Root Mean 
		Squared Error in parameter estimates) as the inclusive strategy, and acceptable coverage.
		As for the non linear set up the same performance was showed by the auxiliary-inclusive 
		and the PCA approaches. In a real dataset example, the authors also find that the PCA 
		strategy grants the least FMI (i.e. the lowest loss in efficiency of parameters estimates 
		due to missing data). \\

	The approach is promising but was evaluated only with very few predictors in the analysis model 
		and 45 auxiliary variables. PCA inclusion of auxiliary variables in the imputation model 
		needs to be tested on actually high dimensional scenario. The method has a few weaknesses:
		the imputer must decide the number of PCs to choose; PCA can only be computed on a complete
		dataset, hence some preliminary imputation needs to be done and the impact of such choices
		on the PCs effectiveness was unexplored; PCA is in general sensitive to outliers.
	}
}

@article{kenward:1998,
annote =
	{
	A detailed review of when and in what sense the Maximum Likelihood
		inference with data missing at random can be considered valid. This is a technical source
		that can help understand the implications of the expected or the observed information matrix, 
		in Maximum Likelihood estimation, for the validity of the standard errors with MAR data.
	}
}

@article{kimEtAl:2005,
annote = 
	{
	The auhtors developed the Local Least Square imputation (LLSImputation) method by which similar 
		k-neighbour genes are selected, then used to estimate a prediction model which is finally 
		employed to predict the missing values. \\

	The article compares LSSImputation with the KNNImputation and SVDimputation proposed by Troyanskaya 
		\text{et al} (2001), and the Bayesian PCA proposed by Oba \textit{et al.} (2003). As the 
		BPCA approach improves on the SVDImputation by incorporating Bayesian optimisation in a 
		PC based method, LLSI improves on KNNI by combining the local similarity structure and 
		the optimisation procedure of least squares. \\

	Finally, the article directly confronts the issue of choosing the optimal number of k-nearest 
		neighbours. In absence of clear theory, the authors propose to empirically identify on 
		a case-by-case basis the value of \textit{k} by repeatedly predicting some artificially 
		imposed missing values and selecting the one that best recover the known fabricated 
		missing values.
	}
}

@article{lohEtAl:2012,
annote = { The authors aim to solve the nonvonvexity problem in the context of
                  high-dimensional sparse linear regression. The authors claim
                  that the estimators often become nonconvex with noisy data in
                  current approaches such as EM algorithm. It can not be
                  guaranteed that the local minimum is the global minimum during
                  the estimation in such cases. The authors propose that
                  projected gradient descent algorithm will produce a sufficient
                  output with a small statistical error, which means an accurate
                  imputation. The authors describe two general theorems that the
                  first theorem provides bounds on the statistical error, and
                  the second theorem provides bounds on the optimisation error
                  from the theoretical aspect which are essential for the
                  estimation. The proposed method is then presented with
                  different sets of noisy data, they all provide that the
                  estimators from the projected gradient descent algorithm have
                  high accuracy. The authors further confirm their predictions
                  in theory illustrations with simulations studies. The authors
                  conclude that although the results from the projected gradient
                  descent remain nonconvex, its accuracy is statistically
                  acceptable. One restriction is that the data are assumed to be
                  sub-Gaussian in the projected gradient descent
                  algorithm. Further study is needed for applying to the more
                  general datasets with noise.  }
}

@article{oba2003bayesian,
  annote = 
  	{
	The authors implemented a Bayesian Principle Component Regression apporach that 
	performs better than KNNImpute and SVDImpute proposed by Troyanskaya2001.
  	}
}

@inproceedings{orchardWoodbury:1972,
annote = 
	{
	The authors provide a technical description of the \textit{Missing Information
		Principle}, as defined through the decomposition of the information 
		matrix for a vector of parameters of interest $\theta$.
		The lost (missing) information is defined as the difference between the 
		information matrix for $\theta$ in the hypothetical complete dataset,
		and the information matrix for $\theta$ obtained with just the
		observed cases. This decomposition is also used to describe the increase
		in variance in the parameter estimates caused by the missing data. \\

	For a more approachable definition of the Missing Information Principle see
		Savalei and Rhemtulla (2012).
	}
}

@article{raghunathanEtAl:2001,
annote = 
	{
	Raghunathan \textit{et al} introduce the SRMI (Sequential Regression Multivariate 
		Imputation) approach to missing data imputation. The approach imputes the missing values 
		on a variable-by-variable basis, by using posterior predictive distributions of the missing 
		data, conditional on the observed data. \\
		
	Apart from describing the principles, strengths, and weakness of the approach, their work also provides 
		detailed instructions on how to perform multiple imputation according to SRMI (see Appendix A for 
		a precise discussion on how to draw from a variety of regression models supported by SRMI). \\

	Two case-studies and one simulation study are contextually presented and they are instrumental in 
		showing how the SMRI approach performs compared to complete-case analysis and the Multiple 
		Imputation based on a joint multivariate model.
	}
}

@article{reiter:2005,
annote = 
	{
	The author proposes the use of CART to generate synthetic data sets (i.e. substituting sensitive observed
		data points with multiply imputed values to avoid disclosure of information). For each of many
		subsamples of the original dataset, the algorithm sequentially grows a tree for each of the k-th 
		sensitive variables using all the other variables ($X, Y_{-k}$) as inputs, and then imputes the 
		values to be replaces (the ``missing data points'') though bootstrap sampling of the $Y_k$ values
		in the leaf where each observation falls. \\

	The literature on using CART and other synthetic data generator methods is closely related to that of missing
		data imputation. However, a few remarks are due. The first body of literature is concerned with
		balancing between guarantying statistical validity of the secondary analyses and reducing the risk of
		sensitive information disclosure, while missing data-handling literature is exclusively concerned
		with the statistical validity issue. This is important because the different goal influences the tree 
		pruning strategy. Furthermore, the concept of missing data mechanism is not relevant
		for the synthetic data literature and replacement (missingness) is forced on either the entire variable
		or just specific ranges of it (e.g. replace (impute) values of income which are greater then a 
		threshold). The counterpart scenario in a missing data context is imputing a fully missing variable with
		especially good initial guesses.
	}
}

@article{robbinsEtAl:2013,
annote = { The authors develop an imputation method specifically for the USDA’s
                  Agricultural Resource Management Survey (ARMS) dataset. The
                  authors argue that current imputation method, a form of mean
                  imputation, does not perform well. The authors first claim
                  that ARMS data is not normally distributed even after a
                  log-transformation and its complexity. The skew normal
                  transformation is then considered for a jointly normal
                  model. The authors illustrate several criteria to select the
                  predictors for the model as well. The authors present a new
                  imputation method through Markov chain Monte Carlo sampling
                  from the joint model. After the initialise the Markov chain,
                  iterative sequential regression (ISR) is needed to generate
                  repeated imputation for more accurate estimation. Finally, the
                  authors apply the method to the ARMS data, it’s shown that the
                  new method outperforms the old (mean imputation) approach
                  significantly. The results are evaluated by the consistency of
                  observed data and imputations through density analysis,
                  posterior predictive p-values, etc. It’s shown that ISR method
                  is efficient in this application. }
}

@article{rubin:1976,
annote = 
	{
	The article defines the conditions under which statistical inference can still be considered proper while
		ignoring the missing data mechanisms. This is the reference source for the technical definitions
		of data Missing at Random (MAR), Observed at Random (OAR), and distinctness of the model parameter 
		of interest (i.e. the object of inference) and the nuisance parameter (i.e. the parameter of the 
		missing data process). Apart from rigorous theorems the article presents easy-to-grasp examples
		of what MAR, OAR, and distinctness mean and imply.
	}
}

@article{rubin:1978,
annote = 
	{
	The article defines the foundations of Multiple Imputation, the procedure that imputes missing values
		reflecting the uncertainty within an imputation model and the sensitivity of inferences to
		different imputation models. The article distinguishes three fundamental tasks in the process
		of creating imputations: a modelling task, that chooses a model for the data, an estimation task,
		that finds the posterior distribution for its model parameters, and the imputation task that takes
		draws from the associated predictive distribution given the observed data.
	}
}

@article{rubin:1996,
annote = 
	{
	In this work, Rubin reviews the state of multiple imputation 18 years after the publication 
		of his seminal works in 1976 and 1978. This paper clarifies the two
		main goals that Multiple Imputation was designed to achieve: the basic objective 
		of allowing the data ultimate users to apply the same analytical methods as if 
		the data had been complete; and the supplemental objective of granting analyses that 
		are statistically valid for a scientific estimand. \\

	The concept of statistical validity is operationalised with clarity by distinguishing between: 
		randomisation validity and confidence validity. \\

	Furthermore, the concept of \textit{proper} multiple imputation is discussed by 
		summarising its main requirements. \\
	
	Finally some criticisms to MI are addressed through the lenses provided by these concepts.
	}
}

@article{savaleiRhemtulla:2012,
annote = 
	{
	This article provides a short yet insightful review of the Missing Information 
		Principle (an interested reader may want to consult Orchard and Woodbury (1972)
		for a more technical presentation of the same concept). According to this principle, 
		the missing information for a parameter estimation procedure due to missing data 
		is equal to the difference between the complete-data information matrix and the 
		observed-data information matrix. \\

	The definition of the Fraction of Missing Information (FMI) is explored
		under both the Maximum Likelihood and the Multiple Imputation framework. The 
		fundamental contribution of this paper is in fact showing how FMI is not a prerogative
		of MI. \\

	A final contribution is the detailed discussion of three different possible interpretations
		of FMI: (relative) loss of (estimation) efficacy, loss of statistical power;
       		width inflation factor (i.e. how much bigger are the confidence intervals).	
	}
}

@article{shah:2014,
annote = 
	{
	The authors propose a modfied MICE apporach that uses a Random Forest to generate the conditional distribution 
		from which the imputed values are drawn. The approach is meaningfully different from multiple imputations
		obtained by multiple parallel runs of \textit{missForest}, the random forest imputation method proposed by 
		Stekhoven and B{\"u}hlmann (2012) which results in a deterministic imputation, albeit weighted across 
		different trees.

	The results show good performances of the proposed MICE-Forest in terms of bias and efficiency. In particular,
		the author's method mathced or outperformed the traditional parametric MICE and outperformed their proposed
		MI-missForest approach.
	}
}

@article{shahEtAl:2017,
annote = { The authors aim to develop an imputation method for truncated high
                  dimensional data in metabolomics studies. The authors develop
                  the K-Nearest Neighbors Truncation approach (KNN-TN) which is
                  an extension to the KNN imputation algorithm. The means and
                  standard deviation are estimated first from the truncated
                  normal distribution with KNN-TN approach, they are then used
                  to standardise the metabolites. The data are assumed to miss
                  due to the limit of detection (LOD). Both data missing at
                  random (MAR) and missing not at random (MNAR) are considered
                  under this assumption. The root mean squared error (RMSE) is
                  used to evaluate the performance of the imputation. The
                  authors compare KNN-TN approach with other KNN approaches in a
                  simulation study with different data set sizes and real
                  metabolomics studies.  When dealing with data missing at
                  random combined with an LOD, the imputation results show that
                  KNN-TN approach has the lowest RMSEA compared to KNN approach
                  based on the Euclidean (KNN-EU) and Correlation (KNN-CR). In
                  addition, all three KNN approaches perform better than the
                  standard methods such as zero, minimum and mean
                  imputation. The authors claim that sample size is
                  important. When the sample is too small, the KNN-TN performs
                  not better than KNN-CR approach due to the poor estimation.  }
}

@article{songBelin:2004,
annote = 
	{
	The authors propose a multiple imputation algorithm based on a common factor imputation model. The idea is
		that through specifying a common factor model with uninformative priors on the factor loadings and the
		other parameters, it is possible to predict (impute) the missing values by sampling from a conditional
		distribution that takes into account as much information as possible from the data, without need to
		estimate an excessive number of parameters.

	The approach shows low bias and good coverage properties, however its implementation is restricted to a high-
		dimensional multivariate normal data. Furthermore, this method requires that the true number of components
		is known.
	}
}

@article{stekhovenBuhlmann:2011,
annote = 
	{
	The authors propose an imputation algorithm that exploits the advantages of random forests over traditional 
		CARTs to improve the accuracy of the imputed missing values. Their imputation algorithm sequentially 
		imputes variables $y_s$ with $s=1,...,p$ and $p$ number of variables afflicted by missingness, 
		ordered by ascending percentages of missing values, by fitting a random forest to predict the observed 
		part of $y_s$ with the corresponding observed $x$s, and imputing the missing part of $y_s$ with 
		predicted values obtained by applying the trained random forest to the corresponding observed $x$s.
	}
}

@article{troyanskayaEtAl:2001,
annote = 
	{
	In the context of DNA microarrey studies with missing data, the authors show the better performance of a 
		k-nearest neighbour imputation method (KNNImpute), over an SVD-based regression imputation method
		(SVDImpute), and mean and zero imputation. \\

	The KNNImpute algorithm outperforms all the other methods granting (1) less deterioration in performance
		with increasing percentage of missingness, (2) robustness to the type of data considered (time-series 
		or not, noisy or not), (3) less sensitivity to the number of parameters used (the choice of \textit{k}, 
		the number of nearest neighbours considered for KNNImpute and the number of most significant eigengenes
		selected for SVDImpute). \\

	The generalisability of these results is somewhat hindered by two methodological choices:
	\begin{itemize}
		\item the missing data mechanisms considered is MCAR;
		\item the metric used to identify the better method is the Root Mean Squared error, 
			a standardised difference between the true data points values and the imputed ones.
			As many contributions have highlighted (see Rubin, 1996; de Andrade Silva \textit{et al.}, 2013), the 
			goal of missing data handling procedures is not necessarly recovering the true missing 
			values but rather granting the statistical validity of the analyses performed on a 
			dataset afflicted by missing data.
	\end{itemize}
	}
}

@article{wallaceEtAl:2010,
annote = 
	{
	The authors propose a multiple imputation decision tree method that includes uncertainty regarding
		the imputed values. This is achieved by slightly modifying the single imputation tree-based algorithm 
		proposed by Conversano and Siciliano (2003): the tree is grown M times, and, after the first 
		iteration, the imputed variables are considered as possible candidates for the split as well 
		as the complete ones; once the best splitting variable and values are chosen, the algorithm 
		classifies the cases with missing values on the target variable, based on their observed 
		values on the other features, in a given node, and imputes that node mean value of the target feature. 
		Each time a value is imputed with the node's mean, a random error is added to it. As a result 
		each missing data point is filled with M different values, allowing the algorithm account for 
		uncertainty of the imputed value.
	}

}

@article{wasitoMirkin:2005,
annote = 
	{
	The authors introduce the INImpute approach, an algorithm that combines an iterative SVD-based 
		least-square imputation with a nearest neighbour approach. Missing values are imputed 
		first globally (i.e. considering the entire dataset) through an Iterative Majorization Least Square 
		algorithm (an SVD/PCR-based imputation method with 4 principal components, p=4). Then, a kNN algorithm 
		is used to select k-nearest-neighbours to an instance that had a missing value for a particular variable, 
		and replaces the previously (globally) imputed value with a new one found with anoter IMLS run only among the
		nearest-neighbours (and with p=1). \\

	The theoretical properties of this approach are not discussed but the authors do show the superiority
		of INImpute to regular kNN and other ILS approaches in a variety of scenarios.
	}
}

@article{wasitoMirkin:2006,
annote = 
	{
	The authors extend the comparison of the different least-squares imputation techniques performed by Wasito 
		and Mirkin (2005) to accommodate for different missing data mechanisms (MCAR, MNAR and a merged data 
		missingness). \\

	The results mainly show that all versions of Nearest-Neighbour-based least-squares imputations presented
		outperform the global versions. Furthermore, the global-local INI (IMLS-NN-IMLS) approach wins 
		in almost all contexts except when only the local version (the NN-IMLS) wins.

	The article also includes a detailed description of a data generating process according to the Neural Network 
		NetLab framework, and of the missing data patterns generation.
	}
}

@article{whiteEtAl:2011,
annote = 
	{
	
	White's contribution provides comparative guidelines and recommendations 
		on different aspects of MI:

	\begin{itemize}
	
		\item Handling different dependent variable distributions. In particular, there is
			a helpful section dealing with how to handle imputation for skewed continuous 
			variables.

		\item The question of which variables to include is addressed along with the issue 
			of preserving all the relationships included in the analysis model, when 
			defining the imputation model. Three main ways of dealing with this issues 
			are presented (i.e. passive approach, improved passive approach using PMM, 
			and JAV).

		\item Number of imputations – The authors criticise the \textit{efficiency argument} (that
			usually leads to the rule of thumb  \textit{m} = 5 is adequate for FMI $\leq$ .25) through
			the \textit{replicability argument}, centring the discussion around the decision of \textit{m}
			in terms of Monte Carlo errors. The author's rule of thumb is that \textit{m} $\geq$ \% of 
			incomplete cases.
		
		\item Limitations and pitfalls of MI – Apart from the lack of theoretical background,
			the authors discuss the following pitfalls: perfect prediction (potentially a problem
			when the dependent variable is categorical); sensitivity to MAR violation, non-convergence 
			issues; and the problem of too many variables.
		
	\end{itemize}
	
	}
}

@article{vanBuurenEtAl:2006,
annote = 
	{
	Different algorithms to perform Multiple Imputation by FCS, according to the
		type of dependent variables, are presented in Appendix A of this paper.
		Appendix B describes a method to generate nonmonotone multivariate
		missing data under MAR. \\

	In the main text, after a concise introduction to imputation by FCS, a brief but rigorous 
		definition of \textit{compatibility} between conditional distributions is given. 
		A good technical complement for an interested read is Arnold (2001). 
		Simulations and study cases are then discussed 
		in detail for both univariate and multivariate missing data scenarios, accounting for different
		variable types (i.e. continuous, dichotomous, and polytomous). These studies
		highlight the performances of FCS multiple imputation as compared to 
		complete-case analysis in terms of bias and coverage of the confidence intervals. \\

	Finally, a simulation study is performed to assess the consequences of (in)compatibility.
		This is just an exemplifying scenario: not all possible incompatibility schemes can
		be considered. Yet, it does provide compelling evidence in favour of FCS being 
		robust to (clear) incompatibility.
	}
}

@article{vonHippel:2009,
annote = 
	{
	Transformed variables such as squared and interaction terms need special attention
		when Multiple Imputation is performed. This article compares two main methods
		to impute them: the \textit{transform then impute} and the \textit{impute then transform} 
		approach. \\

	Through extreme missing data scenarios (100\% missignes), and example data analyses, the author
		shows that the \textit{transform then impute} method is the one that best preserves
		the mean and covariance structure of the original data and provides unbiased point
		estimates of regression estimates. As a result, he strongly recommends such method. \\

	Some variants of these methods are included in the comparison, namely \textit{passive imputation},
       		a more sophisticated but equally flawed version of the \textit{impute then transform} approach,	
		and \textit{stratify, then impute} for interactions between categorical and a continuous
		variables. \\

	The main setup considered in this study is that of a linear regression model fitted to a
		dataset with MAR missing data. However, the author addresses the extension of
		the claims to models for binary dependent variables.	
	}
}

@article{xuEtAl:2016,
annote = 
	{
	The authors implemented a Multiple Imputation method using Bayesian Additive Regression Tree prediction
		algorithm proposed by Chipman \textit{et al} (2010) and Bayesian CART. Through BART, a set of 
		different regression tress contributes to defining the predictive distribution of the missing values 
		given the observed ones. For each continuous variable with missing values, a full conditional 
		distribution is specified as a normal distribution centered around the sum-of-trees resulting from 
		the BART model and draws are done with a Metropolis-Hastings algorithm (for binary variables a similar 
		procedure is followed assuming continuous latent variables, while Bayesian CARTs are used for 
		polytomous variables). Interestingly, the BART procedure in a way bypasses the modelling task as 
		defined by Rubin 1978, as MICE does. However, using BART to specify the univariate conditional 
		distributions assures that the imputation model corresponds to a valid joint model. \\

	The approach proved to work similarly or better than standard MICE, and MICE with CART, and showed interesting 
		desirable features that grant it an edge over other methods. Apart from the greater theoretical soundness,  
		multiple imputation using BART and Bayesian CART proved to be more robust to different variable imputation 
		orderings compared to other univariate sequential conditional imputation methods, such as MICE. Another 
		desirable feature is the potential for variables selection that is inherent in the BART algorithm. 
		However, the present paper did not explore such feature.
	}
}

@article{zhang:2012,
annote = 
	{
	The author introduces the Gray k-Nearest-Neighbour (GkNN) algorithm as an improvement to the traditional 
		kNNImpute algorithm (see Troyanskaya \textit{et al.} 2001). This new version uses a Gray Relation
		Grade measure of similarity, from Gray Relational Analysis, which easily 
		accommodates for both numerical and categorical input variables. \\

	Furthermore, GkNN is an iterative algorithm rooted in the EM framework, and the author claims that such feature makes the
		algorithm able to account for the uncertainty related to the imputation procedure. However, it is not explicit how
		this algorithm takes into account the additional uncertainty regarding parameters estimates. \\

	Finally, the GkNN algorithm
		uses all the information included in the dataset by including in the imputation of an instance 
		\text{i}, the observed and imputed values of other instances that have missing values.
	}
}

@article{zhaoLong:2016,
annote = 
	{
	The authors discuss the used of chained equations regularised regression imputation that enables missing data handling 
		within the Multiple Imputation framework with high-dimensional data ($p > n$). Three main approaches are proposed:
		a direct use of regularised regression on multiple bootstrapped datasets (DURR), an indirect use of it, and finally
		a Bayesian lasso approach. \\

	The method BLasso method seems to outperform the other ones. Furthermore, it can easily be extended to general missing 
		data patters (the study only shows results for univariate missing data), while the other methods do not share such
		feature. However, the BLasso approach becomes computationally extremely intensive (even infeasible) when many 
		variables are afflicted by missing values.
	}
}
