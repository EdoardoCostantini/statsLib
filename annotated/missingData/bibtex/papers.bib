%%% Title:    Missing Data Journal Articles
%%% Author:   Kyle M. Lang
%%% Created:  2019-09-13
%%% Modified: 2019-10-21

@article{allison:1987,
title = {Estimation of linear models with incomplete data},
author = {Allison, Paul D.},
journal = {Sociological Methodology},
volume = {17},
number = {1},
pages = {71--103},
year = {1987}
}

@article{anderson:1957,
author = {Anderson, T. W.},
title = {Maximum Likelihood Estimates for a Multivariate Normal Distribution when some Observations are Missing},
journal = {Journal of the American Statistical Association},
volume = {52},
number = {278},
pages = {200--203},
year = {1957},
annote = 
	{
	This article provides a concise mathematical workout of Maximum Likelihood estimation of multivariate
		normal models with a monotone missing data pattern (from bivariate to any-variate generalisation).
	}
}

@article{andridgeLittle:2010,
title = {A review of hot deck imputation for survey non-response},
author = {Andridge, Rebecca R and Little, Roderick J A},
journal = {International Statistical Review},
volume = {78},
number = {1},
pages = {40--64},
year = {2010},
publisher = {Wiley Online Library},
annote=
	{
	Andrige and Little point out two main advantages of hot deck imputation methods: it results in a 
		rectangular data and it does not rely on any model specification. Different measures and methods
		for creating the donor pool are reviewed along with how to account for missing data patterns 
		(monotone or \textit{Swiss cheese}) and how to incorporate sampling weights.
	}
}

@article{aydilekArslan:2013,
title = {A hybrid method for imputation of missing values using optimized fuzzy \emph{c}-means with support vector regression and a genetic algorithm},
author = {Aydilek, Ibrahim Berkan and Arslan, Ahmet},
journal = {Information Sciences},
volume = {233},
pages = {25--35},
year = {2013}
}

@article{baraldiEnders:2010,
title = {An introduction to modern missing data analyses},
author = {Baraldi, Amanda N and Enders, Craig K},
journal = {Journal of School Psychology},
volume = {48},
number = {1},
pages = {5--37},
year = {2010},
publisher = {Elsevier}
}

@article{borgoniBerrington:2013,
author = {Borgoni, Riccardo and Berrington, Ann},
title = {Evaluating a sequential tree-based procedure for multivariate imputation of complex missing data structures},
journal = {Quality \& Quantity},
volume = {47},
number = {4},
year = {2013},
pages = {1991--2008},
doi = {10.1007/s11135-011-9638-3},
annote = 
	{
	The authors integrate a sequential (one-variable-at-the-time + data augmentation) tree-based imputation
		algorithm and non-parametric bootstrap. Specifically, given a set of fully observed inputs and 
		a set of target variables with missing values, their approach grows a tree on a complete subset 
		of the instances for each target variable sequentially (starting with the target variable with 
		fewest missing cases, uses the imputed values to augment the original data, and improve imputation 
		of variables with more missing values), and iterates this procedure until some criteria is reached. 
		To account for the added uncertainty due to imputation, the authors apply this method to a large
		number of bootstrap samples from the original dataset. \\

	Considering only categorical variables, the authors compare mode, sequential regressions, a non-iterative
		tree-based imputations to their bootstrap tree-based imputation algorithm. Most notably, the results 
		of the simulation study point out that non-iterative regression trees are worst than simple
		mode imputation in terms of estimation accuracy (bias and efficiency of a true model parameters
		estimates), and that their bootstrap version of an imputing decision tree compensates well for the
		extra variability added by the imputation process.
	}
}

@article{brasMenezes:2007,
title = {Improving cluster-based missing value estimation of DNA microarray data},
author = {Br\'{a}s, L\'{i}gia P and Menezes, Jos\'{e} C},
journal = {Biomolecular engineering},
volume = {24},
number = {2},
pages = {273--282},
year = {2007}
}

@article{burgetteReiter:2010,
author = {Burgette, Lane F. and Reiter, Jerome P.},
title = {Multiple Imputation for Missing Data via Sequential Regression Trees},
journal = {American Journal of Epidemiology},
volume = {172},
number = {9},
year = {2010},
pages = {1070--1076},
doi = {10.1093/aje/kwq260}
}

@article{collinsEtAl:2001,
author = {Collins, L. M. and Schafer, J. L. and Kam, C.},
title = {A comparison of inclusive and restrictive strategies in modern missing data procedures},
journal = {Pscyhological Methods},
year = {2001},
volume = {6},
number = {4},
pages = {330--351},
doi = {10.1037//1082-989X.6.4.330},
annote = 
	{
	Collins \textit{et al} investigate the effects of an inclusive \textit{vs} a restrictive strategy to the use of
		auxiliary variables in the missing data handling model of either a ML (maximum likelihood) 
		or MI (Multiple Imputation) approach. \\

	The authors distinguish between three categories of auxiliary variables based on their 
		correlation with the variable(s) of interest that presents missing values, and the missingness 
		mechanism). Through 4 different simulation setups, the authors study the
		consequences of including or excluding auxiliary variables, from each of these categories, 
		and ultimately provide compelling evidence to prefer an inclusive strategy. \\

	An interesting point is raised regarding the ease of implementation of the inclusive strategy. In particular, 
		the authors point out that, while the inclusion of auxiliary variables is straightforward in the MI 
		framework, current (up to 2001) software implementations of ML methods do no facilitate it. This topic 
		is thoroughly explored by Graham (2003). \\

	A final point stressed by the article is that the consequences of the inclusive or restrictive strategies 
		heavily depend on the type of MAR. For example, in their simulations, Collins \textit{et al} found 
		that the estimated mean of a variable Y of interest was biased when the auxiliary variable Z was 
		excluded from the data handling model, if the probability of missingess was linearly related to Z 
		(MAR-linear); however, that was not the case when the probability of missigness was larger for 
		extreme values of Z or a function of the correlation between Z and Y.
	}
}

@article{conversanoSiciliano:2009,
author = {Conversano, Claudio and Siciliano, Roberta},
title = {Incremental Tree-Based Missing Data Imputation with Lexicographic Ordering},
journal = {Journal of Classification},
volume = {26},
number = {3},
year = {2009},
pages = {361--379},
doi = {10.1007/s00357-009-9038-8},
annote = 
	{
	The authors introduce the Incremental NonParametric Imputation (INPI) algorithm for imputing large datasets 
		with missing values on multiple categorical and/or continuous variables. This approach reorganises the 
		data according to a lexicographic order (arranging the columns and rows of a data matrix so as to concentrate
		the missing values in a corner), grows a decision tree with a FAST algorithm (selecting first the best
		splitting predictor and then the best splitting cut off value), imputes a missing value that minimises
		a given error/decision rule, and augments the complete data before repeating the process for all the 
		other missing values. \\

	The INPI algorithm exploits all of the advantages of decision tree methods (i.e. nonparametric nature, flexibility
		to variable types) and grants an effective imputation method for large datasets. Another advantage of the 
		is that the algorithm can be easily extended to include imputation model uncertainty (Multiple 
		Imputation). However, it must be noticed that this algorithm assumes a MAR mechanisms and that there 
		is at least one completely observed variable in the original dataset.
	}
}

@article{d'ambrosioEtAl:2012,
author = {D'Ambrosio, Antonio and Aria, Massimo and Siciliano, Roberta},
title = {Accurate Tree-based Missing Data Imputation and Data Fusion within the Statistical Learning Paradigm},
journal = {Journal of Classification},
volume = {29},
number = {2},
year = {2012},
pages = {227--258},
doi = {10.1007/s00357-012-9108-1},
annote = 
	{
	The imputation algorithm proposed in this article improves on the INPI algorithm proposed by Conversano
		and Siciliano (2009) in two ways: by incrementally imputing one variable at the time, it reduces 
		the computational effort compared to the imputation of single missing data points one at the time;
		by using the ensemble learning classifier AdaBoost, instead of single decision tree, the new 
		algorithm is more accurate, in terms of prediction error. \\

	This article also integrates this imputation approach in a ``data fusion'' problem. This could be 
		interesting for imputation problems with datasets gathered according to planned missing data 
		designs. \\

	The article interestingly includes the MICE approach as one of the imputation methods compared. However, 
		it does not provide a valid measure of comparison between MICE and BINPI. The authors stress how
		the performance of a BINPI algorithm should be evaluated based on its ability to recover the 
		missing values (according to the Statistical Learning Theory framework). However, MICE is an imputation
		strategy grounded in the stochastic framework proposed by Rubin. Hence, MICE should be 
		evaluated not in terms of the ability to recover missing values (prediction error), but in terms of
		how statistically valid the analysis that it allows to perform are.
	}
}

@inproceedings{deAndradeSilvaHruschka:2009,
title={EACImpute: an evolutionary algorithm for clustering-based imputation},
author={de Andrade Silva, Jonathan and Hruschka, Eduardo R},
booktitle={2009 Ninth International Conference on Intelligent Systems Design and Applications},
pages={1400--1406},
year={2009},
organization={IEEE}
}

@article{deAndradeSilvaHruschka:2013,
author = {de Andrade Silva, Jonathan and Hruschka, Eduardo Raul},
title = {An experimental study on the use of nearest neighbor-based imputation algorithms for classification tasks},
journal = {Data \& Knowledge Engineering},
volume = {84},
year = {2013},
pages = {47--58},
doi = {10.1016/j.datak.2012.12.006},
annote = 
	{
	The article compares 5 popular Nearest-Neighbour algorithms based on both their prediction accuracy and 
		the classification bias. The results show that in a MCAR context, the Iterative KNNImpute algorithm proposed
		by Bràs and Menezes (2007) outperforms all other methods, according to both the Normalised Root Mean Squared 
		Error (NRMSE) and the Average Correct Classification Rate (ACCR) criterion. However, in a MAR scenario,
		the KNNI proposed by Troyanskaya et al 2001, the Sequential KNNI proposed by Kim et al (2004), and the 
		EACI proposed by de Andrade Silva and Hruschka (2009) perform best. \\

	One of the clearest contribution of the article is showing that
		better prediction accuracy does not necessarily imply a better modelling performance (i.e. an algorithm
		might ``recover'' the missing values better, but another algorithm might provide a better estimation
		of the relationship between attributes). This result motivates a departure from the exclusive use of
		the Normalised Root Mean Squared Error measure of prediction accuracy as the criteria to evaluate an imputation
		algorithm. It must be noted that this article is concerned exclusively with a classification task.
	}
}

@article{dempsterEtAl:1977,
title = {Maximum likelihood from incomplete data via the EM algorithm},
author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
pages = {1--38},
year = {1977},
doi = {10.2307/2984875},
annote = 
	{
	The EM algorithm is introduced in this paper as an approach to iteratively 
		compute maximum likelihood estimates when data is incomplete. \\
		
	The algorithm is introduced first through a numerical example involving discrete variables,
		then, formally, for the case in which the complete data distribution belongs to the 
		exponential family. Finally, it is extended for any distributional family.
		
	The article extensively discusses the general properties of the EM algorithm with great
		attention to the technical details. In section 3, the authors provide the mathematical
		justification for the effectiveness of the algorithm in finding the maximum likelihood
		estimate of a vector parameter when the complete-data likelihood is unkwon/intractable.
		
	Finally, the application of the EM algorithm is discursively exemplified in different scenarios
		such as missing data, and mixture modelling.
	}
}

@article{dengEtAl:2016,
title = {Multiple imputation for general missing data patterns in the presence of high-dimensional data},
author = {Deng, Yi and Chang, Changgee and Ido, Moges Seyoum and Long, Qi},
journal = {Scientific reports},
volume = {6},
pages = {21689},
year = {2016},
publisher = {Nature Publishing Group},
doi = {10.1038/srep21689}
}

@article{drechslerReiter:2011,
author = {Drechsler, J\"{o}rg and Reiter, Jerome P.},
title = {An empirical evaluation of easily implemented, nonparametric methods for generating synthetic datasets},
journal = {Computational Statistics and Data Analysis},
volume = {55},
number = {12},
year = {2011},
pages = {3232--3243},
doi = {10.1016/j.csda.2011.06.006}
}

@article{enders:2001-mlr,
title = {The performance of the full information maximum likelihood estimator in multiple regression models with missing data},
author = {Enders, Craig K},
journal = {Educational and Psychological Measurement},
volume = {61},
number = {5},
pages = {713--740},
year = {2001},
publisher = {Sage Publications Sage CA: Thousand Oaks, CA}
}

@article{enders:2001-primer,
title = {A primer on maximum likelihood algorithms available for use with missing data},
author = {Enders, Craig K},
journal = {Structural Equation Modeling},
volume = {8},
number = {1},
pages = {128--141},
year = {2001},
publisher = {Taylor \& Francis}
}

@article{enders:2013,
title = {Dealing with missing data in developmental research},
author = {Enders, Craig K},
journal = {Child Development Perspectives},
volume = {7},
number = {1},
pages = {27--31},
year = {2013},
publisher = {Wiley Online Library}
}

@article{endersBandalos:2001,
title = {The relative performance of full information maximum likelihood estimation for missing data in structural equation models},
author = {Enders, Craig K and Bandalos, Deborah L},
journal = {Structural Equation Modeling},
volume = {8},
number = {3},
pages = {430--457},
year = {2001},
publisher = {Taylor \& Francis}
}

@article{fessantMidenet:2002,
author = {Fessant, Fran\c{c}oise and Midenet, Sophie},
title = {Self-Organising Map for Data Imputation and Correction in Surveys},
journal = {Neural Computing and Applications},
volume = {10},
number = {4},
year = {2002},
pages = {300--310},
doi = {10.1007/s005210200002}
}

@article{gabrys:2002,
title = {Neuro-fuzzy approach to processing inputs with missing values in pattern recognition problems},
author = {Gabrys, Bogdan},
journal = {International Journal of Approximate Reasoning},
volume = {30},
number = {3},
pages = {149--179},
year = {2002}
}

@article{garcia-laencinaEtAl:2009,
author = {Garc\'{i}a-Laencina, Pedro J. and Sancho-G\'{o}mez, Jos\'{e}-Luis and Figueiras-Vidal, An\'{i}bal R. and Verleysen, Michel},
title = {\emph{K} nearest neighbours with mutual information for simultaneous classification and missing data imputation},
journal = {Neurocomputing},
volume = {72},
number = {7--9},
year = {2009},
pages = {1483--1493},
doi = {10.1016/j.neucom.2008.11.026},
annote =
	{
	The authors introduce an improved version of the KNNImpute algorithm proposed by Troyanskaya 
		\textit{et al.} 2001. Their proposed strategy is called MI-KNNImpute, a somewhat confusing
		label: the ``MI'' portion does not refer to ``multiple imputation'' but to the concept of
		Mutual Information (i.e. the reduction of the uncertainty of a variable when another one is
		known). \\
	
	The authors show with different incomplete datasets that the performance of the classification task, performed 
		by a KNN algorithm that uses a distance measure of Euclidian form and one that uses MI, is 
		improved by first imputing the datasets with the MI-KNNImpute algorithm compared to the standard 
		KNNImpute. \\

	The improved performance is achieved by including some information regarding the classification task
		in the imputation phase, through the use of MI as a measure of distance between the target 
		class variable and the other input attributes. The use of MI in the algorithm effectively 
		weights the importance of each attribute for the imputation of the target class variable.
	}
}

@article{garcia-laencinaEtAl:2010,
author = {Garc\'{i}a-Laencina, Pedro J. and Sancho-G\'{o}mez, Jos\'{e}-Luis and Figueiras-Vidal, An\'{i}bal R.},
title = {Pattern classification with missing data: {A} review},
journal = {Neural Computing \& Applications},
volume = {19},
number = {2},
year = {2010},
pages = {263--282},
doi = {10.1007/s00521-009-0295-6}
}

@article{garcia-laencinaEtAl:2013,
author = {Garc\'{i}a-Laencina, Pedro J. and Sancho-G\'{o}mez, Jos\'{e}-Luis and Figueiras-Vidal, An\'{i}bal R.},
title = {Classifying patterns with missing values using Multi-Task Learning perceptrons},
journal = {Expert Systems with Applications},
volume = {40},
number = {4},
year = {2013},
pages = {1333--1341},
doi = {10.1016/j.eswa.2012.08.057}
}

@article{gheyasSmith:2010,
author = {Gheyas, Iffat A. and Smith, Leslie S.},
title = {A neural network-based framework for the reconstruction of incomplete data sets},
journal = {Neurocomputing},
volume = {73},
number = {16--18},
year = {2010},
pages = {3039--3065},
doi = {10.1016/j.neucom.2010.06.021}
}

@article{graham:2003,
author = {Graham, John W.},
title = {Adding Missing-Data-Relevant Variables to FIML-Based Structural Equation Models},
journal = {Structural Equation Modeling: A Multidisciplinary Journal},
volume = {10},
number = {1},
pages = {80--100},
year = {2003},
doi = {10.1207/S15328007SEM1001_4},
annote =
	{
	The author effectively shows, through multiple simulation studies, that including auxiliary 
		variables, when using a FIML (Full Information Maximum Likelihood) approach to 
		handling and analysing datasets with missing data, can be done relatively easily
		under the structural equation modelling framework. \\

	With this paper, Graham introduces the \textit{Saturated Correlated model} and 
		\textit{the extra dv model} to include auxiliary variables in a SEM model. Ultimately,
		he showed that it is possible to include auxiliary variables in a ML missing data 
		handling procedure, without affecting the substantive model (that is to say obtaining 
		the same parameter estimates, standard errors, and estimates of quality of fit of a 
		substantive model that does not include them).
	}
}

@article{graham:2009,
title={Missing data analysis: Making it work in the real world},
author={Graham, John W.},
journal={Annual review of psychology},
volume={60},
pages={549--576},
year={2009}
}

@article{guptaLam:1996,
author = {Gupta, Amit and Lam, Monica S.},
title = {Estimating Missing Values Using Neural Networks},
journal = {The Journal of the Operational Research Society},
volume = {47},
number = {2},
year = {1996},
pages = {229--238},
doi = {10.2307/2584344}
}

@article{heBelin:2014,
title = {Multiple imputation for high-dimensional mixed incomplete continuous and binary data},
author = {He, Ren and Belin, Thomas},
journal = {Statistics in Medicine},
volume = {33},
number = {13},
pages = {2251--2262},
year = {2014},
publisher = {Wiley Online Library},
doi = {10.1002/sim.6107}
}

@article {honakerKing:2010,
author = {Honaker, J. and King, G.},
title = {What to Do about Missing Values in Time-Series Cross-Section Data},
journal = {American Journal of Political Science},
volume = {54},
number = {2},
publisher = {Blackwell Publishing Inc.},
doi = {10.1111/j.1540-5907.2010.00447.x},
pages = {561--581},
year = {2010}
}

@article{howardEtAl:2015,
title = {Using principal components as auxiliary variables in missing data estimation},
author = {Howard, Waylon J and Rhemtulla, Mijke and Little, Todd D},
journal = {Multivariate Behavioral Research},
volume = {50},
number = {3},
pages = {285--299},
year = {2015},
publisher = {Taylor \& Francis}
}

@article{iacusPorro:2007,
author = {Iacus, Stefano M. and Porro, Giuseppe},
title = {Missing data imputation, matching and other applications of random recursive partitioning},
journal = {Computational Statistics \& Data Analysis},
volume = {52},
number = {2},
year = {2007},
pages = {773--789},
doi = {10.1016/j.csda.2006.12.036}
}

@article{jerezEtAl:2010,
author = {Jerez, Jos\'{e} M. and Molina, Ignacio and Garc\'{i}a-Laencina, Pedro J. and Alba, Emilio and Ribelles, Nuria and Mart\'{i}n, Miguel and Franco, Leonardo},
title = {Missing data imputation using statistical and machine learning methods in a real breast cancer problem},
journal = {Artificial Intelligence in Medicine},
volume = {50},
number = {2},
year = {2010},
pages = {105--115},
doi = {10.1016/j.artmed.2010.05.002}
}

@article{junninenEtAl:2004,
author = {Junninen, Heikki and Niska, Harri and Tuppurainen, Kari and Ruuskanen, Juhani and Kolehmainen, Mikko},
title = {Methods for imputation of missing values in air quality data sets},
journal = {Atmospheric Environment},
volume = {38},
number = {18},
year = {2004},
pages = {2895--2907},
doi = {10.1016/j.atmosenv.2004.02.026}
}

@article{kangYusof:2012,
author = {Kang, Ho Ming and Yusof, Fadhilah},
title = {Application of Self-Organizing Map {(SOM)} in Missing Daily Rainfall Data in Malaysia},
journal = {International Journal of Computer Applications},
volume = {48},
number = {5},
year = {2012},
pages = {23--28},
doi = {10.5120/7345-0160}
}

@article{kimEtAl:2005,
title = {Missing value estimation for {DNA} microarray gene expression data: local least squares imputation},
author = {Kim, Hyunsoo and Golub, Gene H. and Park, Haesun},
journal = {Bioinformatics},
volume = {21},
number = {2},
pages = {187--198},
year = {2005},
doi = {10.1093/bioinformatics/bth499},
annote = 
	{
	The auhtors developed the Local Least Square imputation (LLSImputation) method by which similar 
		k-neighbour genes are selected, then used to estimate a prediction model which is finally 
		employed to predict the missing values. \\

	The article compares LSSImputation with the KNNImputation and SVDimputation proposed by Troyanskaya 
		\text{et al} (2001), and the Bayesian PCA proposed by Oba \textit{et al.} (2003). As the 
		BPCA approach improves on the SVDImputation by incorporating Bayesian optimisation in a 
		PC based method, LLSI improves on KNNI by combining the local similarity structure and 
		the optimisation procedure of least squares. \\

	Finally, the article directly confronts the issue of choosing the optimal number of k-nearest 
		neighbours. In absence of clear theory, the authors propose to empirically identify on 
		a case-by-case basis the value of \textit{k} by repeatedly predicting some artificially 
		imposed missing values and selecting the one that best recover the known fabricated 
		missing values.
	}
}

@article{kimEtAl:2004,
title = {Reuse of imputed data in microarray analysis increases imputation efficiency},
author = {Kim, Ki-Yeol and Kim, Byoung-Jin and Yi, Gwan-Su},
journal = {BMC bioinformatics},
volume = {5},
number = {160},
year = {2004},
doi = {10.1186/1471-2105-5-160}
}

@article{lakshminarayanEtAl:1999,
author = {Lakshminarayan, Kamakshi and Harp, Steven A. and Samad, Tariq},
title = {Imputation of Missing Data in Industrial Databases},
journal = {Applied Intelligence},
volume = {11},
number = {3},
year = {1999},
pages = {259--275},
doi={10.1023/A:1008334909089}
}

@article{langLittle:2018,
title = {Principled missing data treatments},
author = {Lang, Kyle M and Little, Todd D},
journal = {Prevention Science},
volume = {19},
number = {3},
pages = {284--294},
year = {2018},
publisher = {Springer}
}

@article{langWu:2017,
author = {Kyle M. Lang and Wei Wu},
title = {A Comparison of Methods for Creating Multiple Imputations of Nominal Variables},
journal = {Multivariate Behavioral Research},
volume = {52},
number = {3},
pages = {290-304},
year = {2017},
publisher = {Routledge},
doi = {10.1080/00273171.2017.1289360}
}

@article{liaoEtAl:2014,
title = {Missing value imputation in high-dimensional phenomic data: {Imputable} or not, and how?},
author = {Liao, Serena G and Lin, Yan and Kang, Dongwan D and Chandra, Divay and Bon, Jessica and Kaminski, Naftali and Sciurba, Frank C and Tseng, George C},
journal = {BMC bioinformatics},
volume = {15},
number = {1},
pages = {346},
year = {2014},
publisher = {BioMed Central}
}

@article{little:1988,
title = {Missing-data adjustments in large surveys},
author = {Little, Roderick J. A.},
journal = {Journal of Business \& Economic Statistics},
volume = {6},
number = {3},
pages = {287--296},
year = {1988}
}

@article{littleEtAl:2013,
author = {Little, Todd D. and Jorgensen, Terrence D. and Lang, Kyle M. and Moore, E. W. G.},
title = {On the Joys of Missing Data},
journal = {Journal of Pediatric Psychology},
year = {2013},
pages = {1--12},
doi = {10.1093/jpepsy/jsto48}
}

@article{littleSchluchter:1985,
title = {Maximum likelihood estimation for mixed continuous and categorical variables with missing values},
author = {Little, R. J. A. and Schluchter, M. D.},
journal = {Biometrika},
volume = {72},
pages = {497--512},
year = {1985}
}

@article{luengoEtAl:2010,
author = {Luengo, Juli\'{a}n and Garc\'{i}a, Salvador and Herrara, Francisco},
title = {A study on the use of imputation methods for experimentation with Radial Basis Function Network classifiers handling missing attribute values: {The} good synergy between {RBFNs} and {EventCovering} method},
journal = {Neural Networks},
volume = {23},
number = {3},
year = {2010},
pages = {406--418},
doi = {10.1016/j.neunet.2009.11.014}
}

@article{morganSonquist:1963,
title = {Problems in the analysis of survey data, and a proposal},
author = {Morgan, James N. and Sonquist, John A.},
journal = {Journal of the American Statistical Association},
volume = {58},
number = {302},
pages = {415--434},
year = {1963}
}

@article{nanniEtAl:2012,
author = {Nanni, Loris and Lumini, Alessandra and Brahnam, Sheryl},
title = {A classifier ensemble approach for the missing feature problem},
journal = {Artificial Intelligence in Medicine},
volume = {55},
number = {1},
year = {2012},
pages = {37--50},
doi = {10.1016/j.artmed.2011.11.006}
}

@article{nordbotten:1995,
title = {Editing Statistical Records by Neural Networks},
author = {Nordbotten, Svein},
journal = {Journal of Official Statistics},
volume = {11},
number = {4},
pages = {391--411},
year = {1995},
url = {http://hdl.handle.net/11250/181393}
}

@article{nordbotten:1996,
author = {Nordbotten, Svein},
title = {Neural Network Imputation Applied to the Norwegian 1990 Population Census Data},
journal = {Journal of Official Statistics},
volume = {12},
number = {4},
year = {1996},
pages = {385--401},
url = {http://hdl.handle.net/11250/178156}
}

@article{oba2003bayesian,
  title={A Bayesian missing value estimation method for gene expression profile data},
  author={Oba, Shigeyuki and Sato, Masa-aki and Takemasa, Ichiro and Monden, Morito and Matsubara, Ken-ichi and Ishii, Shin},
  journal={Bioinformatics},
  volume={19},
  number={16},
  pages={2088--2096},
  year={2003},
  publisher={Oxford University Press},
  annote = 
  	{
	The authors implemented a Bayesian Principle Component Regression apporach that 
	performs better than KNNImpute and SVDImpute proposed by Troyanskaya2001.
  	}
}

@inproceedings{orchardWoodbury:1972,
title = {A missing information principle: {Theory} and applications},
author = {Orchard, Terence and Woodbury, Max A},
booktitle = {Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Theory of Statistics},
year = {1972},
organization = {The Regents of the University of California},
annote = 
	{
	The authors provide a technical description of the \textit{Missing Information
		Principle}, as defined through the decomposition of the information 
		matrix for a vector of parameters of interest $\theta$.
		The lost (missing) information is defined as the difference between the 
		information matrix for $\theta$ in the hypothetical complete dataset,
		and the information matrix for $\theta$ obtained with just the
		observed cases. This decomposition is also used to describe the increase
		in variance in the parameter estimates caused by the missing data. \\

	For a more approachable definition of the Missing Information Principle see
		Savalei and Rhemtulla (2012).
	}
}

@article{piela:2002,
author = {Piela, P.},
title = {Introduction to Self-Organizing Maps Modelling for Imputation---Techniques \& Technology},
journal = {Research in Official Statistics},
volume = {2},
year = {2002},
pages = {5--19}
}

@article{polikarEtAl:2010,
author = {Polikar, Robi and {DePasquale}, Joseph and Mohammed, Hussein Syed and Brown, Gavin and Kuncheva, Ludmilla I.},
title = {{Learn++.MF: A} random subspace approach for the missing feature problem},
journal = {Pattern Recognition},
volume = {43},
number = {11},
year = {2010},
pages = {3817--3832},
doi = {10.1016/j.patcog.2010.05.028}
}

@article{raghunathanEtAl:2001,
title = {A multivariate technique for multiply imputing missing values using a sequence of regression models},
author = {Raghunathan, Trivellore E. and Lepkowski, James M. and Van Hoewyk, John and Solenberger, Peter},
journal = {Survey Methodology},
volume = {27},
number = {1},
pages = {85--96},
year = {2001},
annote = 
	{
	Raghunathan \textit{et al} introduce the SRMI (Sequential Regression Multivariate 
		Imputation) approach to missing data imputation. The approach imputes the missing values 
		on a variable-by-variable basis, by using posterior predictive distributions of the missing 
		data, conditional on the observed data. \\
		
	Apart from describing the principles, strengths, and weakness of the approach, their work also provides 
		detailed instructions on how to perform multiple imputation according to SRMI (see Appendix A for 
		a precise discussion on how to draw from a variety of regression models supported by SRMI). \\

	Two case-studies and one simulation study are contextually presented and they are instrumental in 
		showing how the SMRI approach performs compared to complete-case analysis and the Multiple 
		Imputation based on a joint multivariate model.
	}
}

@article{reiter:2005,
author = {Reiter, J. P.},
title = {Using {CART} to Generate Partially Synthetic, Public Use Microdata},
journal = {Journal of Official Statistics},
volume = {21},
number = {3},
year = {2005},
pages = {7--30}
}

@article{rey-del-castilloCardenosa:2012,
author = {{Rey-del-Castillo}, Pilar and Carde\~{n}osa, Jes\'{u}s},
title = {Fuzzy min--max neural networks for categorical data: application to missing data imputation},
journal = {Neural Computing \& Applications},
volume = {21},
number = {6},
year = {2012},
pages = {1349--1362},
doi = {10.1007/s00521-011-0574-x}
}

@article{rubin:1976,
title = {Inference and missing data},
author = {Rubin, Donald B},
journal = {Biometrika},
volume = {63},
number = {3},
pages = {581--592},
year = {1976},
publisher = {Oxford University Press}
}

@inproceedings{rubin:1978,
title = {Multiple imputations in sample surveys -- {A} phenomenological {Bayesian} approach to nonresponse},
author = {Rubin, Donald B},
booktitle = {Proceedings of the survey research methods section of the {American Statistical Association}},
volume = {1},
pages = {20--34},
year = {1978},
organization = {American Statistical Association}
}

@article{rubin:1986,
title = {Statistical matching using file concatenation with adjusted weights and multiple imputations},
author = {Rubin, Donald B.},
journal = {Journal of Business \& Economic Statistics},
volume = {4},
number = {1},
pages = {87--94},
year = {1986}
}

@article{rubin:1996,
title = {Multiple imputation after 18+ years},
author = {Rubin, Donald B},
journal = {Journal of the American Statistical Association},
volume = {91},
number = {434},
pages = {473--489},
year = {1996},
annote = 
	{
	In this work, Rubin reviews the state of multiple imputation 18 years after the publication 
		of his seminal works in 1976 and 1978. This paper clarifies the two
		main goals that Multiple Imputation was designed to achieve: the basic objective 
		of allowing the data ultimate users to apply the same analytical methods as if 
		the data had been complete; and the supplemental objective of granting analyses that 
		are statistically valid for a scientific estimand. \\

	The concept of statistical validity is operationalised with clarity by distinguishing between: 
		randomisation validity and confidence validity. \\

	Furthermore, the concept of \textit{proper} multiple imputation is discussed by 
		summarising its main requirements. \\
	
	Finally some criticisms to MI are addressed through the lenses provided by these concepts.
	}
}

@article{saar-tsechanskyProvost:2007,
author = {Saar-Tsechansky, Maytal and Provost, Foster},
title = {Handling Missing Values when Applying Classification Models},
journal = {Journal of Machine Learning Research},
volume = {8},
year = {2007},
pages = {1217--1250}
}

@article{samadHarp:1992,
author = {Samad, Tariq and Harp, Steven A},
title = {Self-organization with partial data},
journal = {Network: Computation in Neural Systems},
volume = {3},
number = {2},
pages = {205--212},
year = {1992},
doi = {10.1088/0954-898X/3/2/008}
}

@article{savaleiRhemtulla:2012,
title = {On obtaining estimates of the fraction of missing information from full information maximum likelihood},
author = {Savalei, Victoria and Rhemtulla, Mijke},
journal = {Structural Equation Modeling: A Multidisciplinary Journal},
volume = {19},
number = {3},
pages = {477--494},
year = {2012},
publisher = {Taylor \& Francis},
annote = 
	{
	This article provides a short yet insightful review of the Missing Information 
		Principle (an interested reader may want to consult Orchard and Woodbury (1972)
		for a more technical presentation of the same concept). According to this principle, 
		the missing information for a parameter estimation procedure due to missing data 
		is equal to the difference between the complete-data information matrix and the 
		observed-data information matrix. \\

	The definition of the Fraction of Missing Information (FMI) is explored
		under both the Maximum Likelihood and the Multiple Imputation framework. The 
		fundamental contribution of this paper is in fact showing how FMI is not a prerogative
		of MI. \\

	A final contribution is the detailed discussion of three different possible interpretations
		of FMI: (relative) loss of (estimation) efficacy, loss of statistical power;
       		width inflation factor (i.e. how much bigger are the confidence intervals).	
	}
}

@article{schaferGraham:2002,
author = {Joseph L. Schafer and John W. Graham},
title = {Missing Data: Our View of State of the Art},
journal = {Psychological Methods},
volume = {7},
number = {2},
year = {2002},
pages = {147--177},
doi = {10.1037//1082-989X.7.2.147}
}

@article{sharpeSolly:1995,
author = {Sharpe, P. K. and Solly, R. J.},
title = {Dealing with Missing Values in Neural Network-Based Diagnostic Systems},
journal = {Neural Computing \& Applications},
volume = {3},
number = {2},
year = {1995},
pages = {73--77},
doi = {10.1007/BF01421959}
}

@article{silva-ramirezEtAl:2011,
author = {Silva-Ram\'{i}rez, Esther-Lydia and Pino-Mej\'{i}as, Rafael and L\'{o}pez-Coello, Manuel and Cubiles-de-la-Vega, Mar\'{i}a-Dolores},
title = {Missing value imputation on missing completely at random data using multilayer perceptrons},
journal = {Neural Networks},
volume = {24},
number = {1},
year = {2011},
pages = {121--129},
doi = {10.1016/j.neunet.2010.09.008}
}

@article{songBelin:2004,
title = {Imputation for incomplete high-dimensional multivariate normal data using a common factor model},
author = {Song, Juwon and Belin, Thomas R},
journal = {Statistics in Medicine},
volume = {23},
number = {18},
pages = {2827--2843},
year = {2004},
publisher = {Wiley Online Library},
doi = {10.1002/sim.1867}
}

@article{songEtAl:2008,
author = {Song, Qinbao and Shepperd, Martin and Chen, Xiangru and Liu, Jun},
title = {Can \emph{k}-{NN} imputation improve the performance of {C4.5} with small software project data sets? {A} comparative evaluation},
journal = {The Journal of Systems and Software},
volume = {81},
number = {12},
year = {2008},
pages = {2361--2370},
doi = {10.1016/j.jss.2008.05.008}
}

@article{sterneEtAl:2009,
title = {Multiple imputation for missing data in epidemiological and clinical research: {Potential} and pitfalls},
author = {Sterne, Jonathan AC and White, Ian R and Carlin, John B and Spratt, Michael and Royston, Patrick and Kenward, Michael G and Wood, Angela M and Carpenter, James R},
journal = {BMJ},
volume = {338},
pages = {b2393},
year = {2009},
publisher = {British Medical Journal Publishing Group}
}

@article{tannerWong:1987,
title = {The calculation of posterior distributions by data augmentation},
author = {Tanner, Martin A. and Wong, Wing Hung},
journal = {Journal of the American Statistical Association},
volume = {82},
number = {398},
pages = {528--540},
year = {1987}
}

@article{troyanskayaEtAl:2001,
title = {Missing value estimation methods for {DNA} microarrays},
author = {Troyanskaya, Olga and Cantor, Michael and Sherlock, Gavin and Brown, Pat and Hastie, Trevor and Tibshirani, Robert and Botstein, David and Altman, Russ B.},
journal = {Bioinformatics},
volume = {17},
number = {6},
pages = {520--525},
year = {2001},
doi = {10.1093/bioinformatics/17.6.520},
annote = 
	{
	In the context of DNA microarrey studies with missing data, the authors show the better performance of a 
		k-nearest neighbour imputation method (KNNImpute), over an SVD-based regression imputation method
		(SVDImpute), and mean and zero imputation. \\

	The KNNImpute algorithm outperforms all the other methods granting (1) less deterioration in performance
		with increasing percentage of missingness, (2) robustness to the type of data considered (time-series 
		or not, noisy or not), (3) less sensitivity to the number of parameters used (the choice of \textit{k}, 
		the number of nearest neighbours considered for KNNImpute and the number of most significant eigengenes
		selected for SVDImpute). \\

	The generalisability of these results is somewhat hindered by two methodological choices:
	\begin{itemize}
		\item the missing data mechanisms considered is MCAR;
		\item the metric used to identify the better method is the Root Mean Squared error, 
			a standardised difference between the true data points values and the imputed ones.
			As many contributions have highlighted (see Rubin, 1996; de Andrade Silva \textit{et al.}, 2013), the 
			goal of missing data handling procedures is not necessarly recovering the true missing 
			values but rather granting the statistical validity of the analyses performed on a 
			dataset afflicted by missing data.
	\end{itemize}
	}
}

@article{wallaceEtAl:2010,
author = {Wallace, Meredith L. and Anderson, Stewart J. and Mazumdar, Sati},
title = {A stochastic multiple imputation algorithm for missing covariate data in tree-structured survival analysis},
journal = {Statistics in Medicine},
volume = {29},
number = {29},
year = {2010},
pages = {3004--3016},
doi = {10.1002/sim.4079},
annote = 
	{
	The authors propose a multiple imputation decision tree method that includes uncertainty regarding
		the imputed values. This is achieved by slightly modifying the single imputation tree-based algorithm 
		proposed by Conversano and Siciliano (2003): the tree is grown M times, and, after the first 
		iteration, the imputed variables are considered as possible candidates for the split as well 
		as the complete ones; once the best splitting variable and values are chosen, the algorithm 
		classifies the cases with missing values on the target variable, based on their observed 
		values on the other features, in a given node, and imputes that node mean value of the target feature. 
		Each time a value is imputed with the node's mean, a random error is added to it. As a result 
		each missing data point is filled with M different values, allowing the algorithm account for 
		uncertainty of the imputed value.
	}

}

@article{wangEtAl:2006,
title = {Missing value estimation for DNA microarray gene expression data by Support Vector Regression imputation and orthogonal coding scheme},
author = {Wang, Xian and Li, Ao and Jiang, Zhaohui and Feng, Huanqing},
journal = {{BMC} Bioinformatics},
volume = {7},
number = {32},
pages = {1--10},
year = {2006},
doi = {10.1186/1471-2105-7-32}
}

@article{wasitoMirkin:2005,
author = {Wasito, Ito and Mirkin, Boris},
title = {Nearest neighbour approach in the least-squares data imputation algorithms},
journal = {Information Sciences},
volume = {169},
number = {1--2},
year = {2005},
pages = {1--25},
doi = {10.1016/j.ins.2004.02.014},
annote = 
	{
	The authors introduce the INImpute approach, an algorithm that combines an iterative SVD-based 
		least-square imputation with a nearest neighbour approach. Missing values are imputed 
		first globally (i.e. considering the entire dataset) through an Iterative Majorization Least Square 
		algorithm (an SVD/PCR-based imputation method with 4 principal components, p=4). Then, a kNN algorithm 
		is used to select k-nearest-neighbours to an instance that had a missing value for a particular variable, 
		and replaces the previously (globally) imputed value with a new one found with anoter IMLS run only among the
		nearest-neighbours (and with p=1). \\

	The theoretical properties of this approach are not discussed but the authors do show the superiority
		of INImpute to regular kNN and other ILS approaches in a variety of scenarios.
	}
}

@article{wasitoMirkin:2006,
author = {Wasito, Ito and Mirkin, Boris},
title = {Nearest neighbour approach in the least-squares data imputation algorithms with different missing patterns},
journal = {Computational Statistics \& Data Analysis},
volume = {50},
number = {4},
year = {2006},
pages = {926--949},
doi = {10.1016/j.csda.2004.11.009},
annote = 
	{
	The authors extend the comparison of the different least-squares imputation techniques performed by Wasito 
		and Mirkin (2005) to accommodate for different missing data mechanisms (MCAR, MNAR and a merged data 
		missingness). \\

	The results mainly show that all versions of Nearest-Neighbour-based least-squares imputations presented
		outperform the global versions. Furthermore, the global-local INI (IMLS-NN-IMLS) approach wins 
		in almost all contexts except when only the local version (the NN-IMLS) wins.

	The article also includes a detailed description of a data generating process according to the Neural Network 
		NetLab framework, and of the missing data patterns generation. Hence, it is a good reference for anyone
		planning a simulation study to test imputation methods.
	}
}

@article{whiteEtAl:2011,
title = {Multiple imputation using chained equations: {Issues} and guidance for practice},
author = {White, Ian R and Royston, Patrick and Wood, Angela M},
journal = {Statistics in Medicine},
volume = {30},
number = {4},
pages = {377--399},
year = {2011},
publisher = {Wiley Online Library},
annote = 
	{
	
	White's contribution provides comparative guidelines and recommendations 
		on different aspects of MI:

	\begin{itemize}
	
		\item Handling different dependent variable distributions. In particular, there is
			a helpful section dealing with how to handle imputation for skewed continuous 
			variables.

		\item The question of which variables to include is addressed along with the issue 
			of preserving all the relationships included in the analysis model, when 
			defining the imputation model. Three main ways of dealing with this issues 
			are presented (i.e. passive approach, improved passive approach using PMM, 
			and JAV).

		\item Number of imputations – The authors criticise the \textit{efficiency argument} (that
			usually leads to the rule of thumb  \textit{m} = 5 is adequate for FMI $\leq$ .25) through
			the \textit{replicability argument}, centring the discussion around the decision of \textit{m}
			in terms of Monte Carlo errors. The author's rule of thumb is that \textit{m} $\geq$ \% of 
			incomplete cases.
		
		\item Limitations and pitfalls of MI – Apart from the lack of theoretical background,
			the authors discuss the following pitfalls: perfect prediction (potentially a problem
			when the dependent variable is categorical); sensitivity to MAR violation, non-convergence 
			issues; and the problem of too many variables.
		
	\end{itemize}
	
	}
}

@article{vanBuurenEtAl:2006,
title={Fully conditional specification in multivarite imputation},
author={van Buuren, Stef and Brand, J. P. L. and Groothuis-Oudshoorn, C. G. M. and Rubin, D. B.},
journal={Journal of Statistical Computation and Simulation},
year={2006},
volume={76},
number={12},
pages={1049--1064},
annote = 
	{
	Different algorithms to perform Multiple Imputation by FCS, according to the
		type of dependent variables, are presented in Appendix A of this paper.
		Appendix B describes a method to generate nonmonotone multivariate
		missing data under MAR. \\

	In the main text, after a concise introduction to imputation by FCS, a brief but rigorous 
		definition of \textit{compatibility} between conditional distributions is given. 
		A good technical complement for an interested read is Arnold (2001). 
		Simulations and study cases are then discussed 
		in detail for both univariate and multivariate missing data scenarios, accounting for different
		variable types (i.e. continuous, dichotomous, and polytomous). These studies
		highlight the performances of FCS multiple imputation as compared to 
		complete-case analysis in terms of bias and coverage of the confidence intervals. \\

	Finally, a simulation study is performed to assess the consequences of (in)compatibility.
		This is just an exemplifying scenario: not all possible incompatibility schemes can
		be considered. Yet, it does provide compelling evidence in favour of FCS being 
		robust to (clear) incompatibility.
	}
}

@article{vonHippel:2007,
title = {Regression with missing {Ys}: An improved strategy for analyzing multiply imputed data},
author = {Von Hippel, Paul T.},
journal = {Sociological Methodology},
volume = {37},
number = {1},
pages = {83--117},
year = {2007}
}

@article{vonHippel:2009,
title = {How to impute interactions, squares, and other transformed variables},
author = {Von Hippel, Paul T.},
journal = {Sociological Methodology},
volume = {39},
number = {1},
pages = {265--291},
year = {2009},
annote = 
	{
	Transformed variables such as squared and interaction terms need special attention
		when Multiple Imputation is performed. This article compares two main methods
		to impute them: the \textit{transform then impute} and the \textit{impute then transform} 
		approach. \\

	Through extreme missing data scenarios (100\% missignes), and example data analyses, the author
		shows that the \textit{transform then impute} method is the one that best preserves
		the mean and covariance structure of the original data and provides unbiased point
		estimates of regression estimates. As a result, he strongly recommends such method. \\

	Some variants of these methods are included in the comparison, namely \textit{passive imputation},
       		a more sophisticated but equally flawed version of the \textit{impute then transform} approach,	
		and \textit{stratify, then impute} for interactions between categorical and a continuous
		variables. \\

	The main setup considered in this study is that of a linear regression model fitted to a
		dataset with MAR missing data. However, the author addresses the extension of
		the claims to models for binary dependent variables.	
	}
}

@article{yoonLee:1999,
author = {Yoon, Song-Yee and Lee, Soo-Young},
title = {Training Algorithm with Incomplete Data for Feed-Forward Neural Networks},
journal = {Neural Processing Letters},
volume = {10},
number = {3},
year = {1999},
pages = {171--179},
doi={10.1023/A:1018772122605}
}

@article{zhang:2012,
author = {Zhang, Shichao},
title = {Nearest neighbor selection for iteratively \emph{k}{NN} imputation},
journal = {The Journal of Systems and Software},
volume = {85},
number = {11},
year = {2012},
pages = {2541--2552},
doi = {10.1016/j.jss.2012.05.073},
annote = 
	{
	The author introduces the Gray k-Nearest-Neighbour (GkNN) algorithm as an improvement to the traditional 
		kNNImpute algorithm (see Troyanskaya \textit{et al.} 2001). This new version uses a Gray Relation
		Grade measure of similarity, from Gray Relational Analysis, which easily 
		accommodates for both numerical and categorical input variables. \\

	Furthermore, GkNN is an iterative algorithm rooted in the EM framework, and the author claims that such feature makes the
		algorithm able to account for the uncertainty related to the imputation procedure. However, it is not explicit how
		this algorithm takes into account the additional uncertainty regarding parameters estimates. \\

	Finally, the GkNN algorithm
		uses all the information included in the dataset by including in the imputation of an instance 
		\text{i}, the observed and imputed values of other instances that have missing values.
	}
}

@article{zhaoLong:2016,
title = {Multiple imputation in the presence of high-dimensional data},
author = {Zhao, Yize and Long, Qi},
journal = {Statistical Methods in Medical Research},
volume = {25},
number = {5},
pages = {2021--2035},
year = {2016},
publisher = {SAGE Publications Sage UK: London, England},
doi = {10.1177/0962280213511027}
}

@article{zhuKosorok:2012,
author = {Zhu, Ruoqing and Kosorok, Michael R.},
title = {Recursively Imputed Survival Trees},
journal = {Journal of the American Statistical Association},
volume = {107},
number = {497},
year = {2012},
pages = {331--340},
doi = {10.1080/01621459.2011.637468}
}
